{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec09c43",
   "metadata": {},
   "source": [
    "<h1 align=\\\"center\\\"><font color='green'><font size=\\\"6\\\">Natural Language Processing (NLP) </font> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebf8753",
   "metadata": {},
   "source": [
    "<div style =\"background-color: #90EE90;\">.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96092f",
   "metadata": {},
   "source": [
    " - Natural Language Processing (NLP) is a branch of artificial intelligence that enables computers to understand, interpret, and generate human language in a meaningful way. \n",
    "  - It plays a crucial role in various applications, such as sentiment analysis, machine translation, and text summarization, making it easier for machines to interact with human language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d151c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries \n",
    "import re #regular expression operators\n",
    "#re extracts typical words\n",
    "import nltk #top library core librarry for NLp\n",
    "#Natural Language tool kit \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "#corpus = large collection of naturally occuring text\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download NLTK data files \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "#punkt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5436a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample questions \n",
    "questions = [\"What’s ur favorite adventure book!? 📚✨\",\n",
    "           \"Are u a nite owl or an early bird!? 🌙🐦\",\n",
    "          \"What’s your go-to source 4 learning something new!?\"]\n",
    "#this is synthetic reviews \n",
    "#You can get data from web scrapping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13af436",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Tokenization\n",
    "\n",
    "**Definition**: Tokenization is the process of breaking down text into smaller units, known as tokens. These can be words, phrases, or sentences.\n",
    "\n",
    "**Purpose**: By tokenizing the input, we can analyze individual components of a sentence, which aids in understanding the overall meaning.\n",
    "\n",
    "**Example**: The sentence \"I love pizza!\" would be tokenized into the tokens: [\"I\", \"love\", \"pizza\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb07cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_question(que):\n",
    "    tokens = nltk.word_tokenize(que)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b514f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Tokenization\n",
    "#tokenizing each questions into individual words (token)\n",
    "def tokenize_question(que):\n",
    "    tokens = nltk.word_tokenize(que)\n",
    "    return tokens\n",
    "#we are giving number of list while it only needs one \n",
    "#we are only trying to \n",
    "#tokenize all the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions = [tokenize_question(que) for que in questions]\n",
    "print(\"Tokenized Questions:\")\n",
    "for i, tokens in enumerate(tokenized_questions):\n",
    "    #print(f\"Question {i+1}:\", tokens)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bc47f",
   "metadata": {},
   "source": [
    "### 2. Lowercasing\n",
    "**Definition**: Lowercasing involves converting all characters in the text to lowercase.\n",
    "\n",
    "**Purpose**: This ensures consistency and prevents treating the same words differently due to case differences (e.g., \"Pizza\" vs. \"pizza\"). It simplifies the matching process during analysis.\n",
    "\n",
    "**Example**: \"I Love Pizza!\" becomes \"i love pizza!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4315056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all tokens to lowercase for uniformity \n",
    "def lowercase_tokens(tokens):\n",
    "    return [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercased_questions = [lowercase_tokens(tokens) for tokens in tokenized_questions]\n",
    "print(\"\\nLowercased Questions:\")\n",
    "for i, tokens in enumerate(lowercased_questions):\n",
    "    print(f\"Question {i+1}:\", tokens)\n",
    "    #print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9f980",
   "metadata": {},
   "source": [
    "### 3. Removing Stop Words\n",
    "**Definition**: Stop words are common words that usually do not carry significant meaning in text analysis (e.g., \"and\", \"the\", \"is\").\n",
    "\n",
    "**Purpose**: Removing stop words helps focus on the more meaningful words in a sentence, improving processing efficiency and relevancy.\n",
    "\n",
    "**Example**: \"I love to eat pizza.\" would become \"love eat pizza.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use stopwords from urdu as well\n",
    "# Step 3: Removing Stop Words\n",
    "# Stop words are common words like 'is', 'and', 'the' that don't add significant meaning to the text\n",
    "stop_words = set(stopwords.words('english')) # Combining English and Hindi stopwords\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb87761",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_questions = [remove_stop_words(tokens) for tokens in lowercased_questions]\n",
    "print(\"\\nQuestions after Removing Stop Words:\")\n",
    "for i, tokens in enumerate(filtered_questions):\n",
    "    print(f\"Questions {i+1}:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da56f69c",
   "metadata": {},
   "source": [
    "### 4. Removing Punctuation\n",
    "**Definition**: This step involves eliminating punctuation marks from the text.\n",
    "\n",
    "**Purpose**: Punctuation can interfere with tokenization and analysis, so removing it helps in treating words uniformly.\n",
    "\n",
    "**Example**: \"Hello, how are you?\" becomes \"Hello how are you\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728619f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Removing Punctuation and Emojis\n",
    "# Removing punctuation and emojis from tokens\n",
    "def remove_punctuation_and_emojis(tokens):\n",
    "    clean_tokens = [token for token in tokens if token.isalnum()]  # Retain only alphanumeric tokens\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_questions = [remove_punctuation_and_emojis(tokens) for tokens in filtered_questions]\n",
    "print(\"\\nQuestions after Removing Punctuation and Emojis:\")\n",
    "for i, tokens in enumerate(cleaned_questions):\n",
    "    print(f\"Questions {i+1}:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18a789",
   "metadata": {},
   "source": [
    "### 5. Stemming and Lemmatization\n",
    "**Definition**: Both stemming and lemmatization are techniques used to reduce words to their base or root form. Stemming cuts words down (e.g., \"running\" becomes \"run\"), while lemmatization considers the context to convert words to their dictionary form.\n",
    "\n",
    "**Purpose**: These techniques help in standardizing words, allowing models to recognize variations of a word as the same term, which is crucial for understanding intent.\n",
    "\n",
    "**Example**:\n",
    " - Stemming: \"running\" → \"run\"\n",
    " - Lemmatization: \"better\" → \"good\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Stemming and Lemmatization\n",
    "# Stemming reduces words to their base form (e.g., 'running' -> 'run')\n",
    "# Lemmatization reduces words to their dictionary root form (e.g., 'running' -> 'run')\n",
    "\n",
    "# Using NLTK's PorterStemmer for stemming\n",
    "stemmer = PorterStemmer()\n",
    "def apply_stemming(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_questions = [apply_stemming(tokens) for tokens in cleaned_questions]\n",
    "print(\"\\nStemmed Questions:\")\n",
    "for i, tokens in enumerate(stemmed_questions):\n",
    "    print(f\"Questions {i+1}:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3627fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using NLTK's WordNetLemmatizer for lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def apply_lemmatization(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_questions = [apply_lemmatization(tokens) for tokens in cleaned_questions]\n",
    "print(\"\\nLemmatized Questions:\")\n",
    "for i, tokens in enumerate(lemmatized_questions):\n",
    "    print(f\"Questions {i+1}:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d8d84",
   "metadata": {},
   "source": [
    "### 6. Handling Numbers\n",
    "**Definition**: This step involves deciding how to process numerical values in the text. Depending on the application, you might convert numbers to their word forms or leave them as is.\n",
    "\n",
    "**Purpose**: Properly handling numbers ensures that the analysis can incorporate quantitative information effectively.\n",
    "\n",
    "**Example**: \"I have 2 apples\" can remain as is or be transformed to \"I have two apples\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb735f0",
   "metadata": {},
   "source": [
    "### 7. Removing Special Characters\n",
    "**Definition**: Special characters include symbols like @, #, $, etc., that don't contribute to the meaning of the text.\n",
    "\n",
    "**Purpose**: Removing these characters helps to clean the input, ensuring that the focus remains on relevant words.\n",
    "\n",
    "**Example**: \"I love pizza!!!\" becomes \"I love pizza\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a9001",
   "metadata": {},
   "source": [
    "### 8. Text Normalization\n",
    "**Definition**: Text normalization is the overall process of transforming text into a consistent format. This includes lowercasing, removing punctuation, handling numbers, and more.\n",
    "\n",
    "**Purpose**: Normalization prepares text for further analysis by ensuring all input is in a standard format, making it easier to process and understand.\n",
    "\n",
    "**Example**: \"I LOVE PIZZA!!! 2 times.\" would be normalized to \"i love pizza two times\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd0cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
